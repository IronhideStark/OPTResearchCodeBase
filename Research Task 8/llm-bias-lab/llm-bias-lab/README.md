# LLM Bias Lab — Bias Detection in LLM Data Narratives

## Quick Start
1. Create and activate a virtual environment.
2. `pip install -r requirements.txt`
3. Put your Excel dataset in `data/` (e.g., `data/source.xlsx`).
4. Configure `config.yml` (model choices, sampling, paths).
5. Generate prompts: `python scripts/experiment_design.py`
6. Run models and log outputs: `python scripts/run_experiment.py`
7. Analyze: `python scripts/analyze_bias.py`
8. Validate factual claims vs ground truth: `python scripts/validate_claims.py`
9. Draft report from `REPORT.md` template.

## Repository Layout
```
llm-bias-lab/
├── analysis/                # Stats, figures, tables
├── data/                    # Your XLS/XLSX/CSV datasets (ignored in .gitignore ideally)
├── prompts/                 # Autogenerated prompt JSONL/CSV
├── results/
│   ├── raw/                 # Raw model outputs (JSONL)
│   └── clean/               # Normalized tables
├── scripts/                 # CLI scripts
├── REPORT.md                # Final report template
├── requirements.txt
└── README.md
```

## Ethics & Safety
- Anonymize all personally identifying info before any prompt is generated.
- Avoid harmful content. Keep demographic attributes synthetic or public and high‑level.
- Pin model versions and document all parameters.
